{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4_transformer",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0afe103e74bb405fa3d214be675fe6d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_df95b38e933349bf91a36a28138ec041",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_272bd060068046abb2b45b265980672a",
              "IPY_MODEL_0c571c27ad254f29ba2d372be8af914a"
            ]
          }
        },
        "df95b38e933349bf91a36a28138ec041": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "272bd060068046abb2b45b265980672a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_83318fe69b5142acb09c3b0be7ab4d98",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 341,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 341,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ecef91390fc4be99706633e882e01cd"
          }
        },
        "0c571c27ad254f29ba2d372be8af914a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3fe9f2a9b34d413281ffc51dd259cfdc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 341/341 [00:00&lt;00:00, 6.60kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c3d0f0c48c6a49a58beec7b4bb747c46"
          }
        },
        "83318fe69b5142acb09c3b0be7ab4d98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ecef91390fc4be99706633e882e01cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3fe9f2a9b34d413281ffc51dd259cfdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c3d0f0c48c6a49a58beec7b4bb747c46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e9d7aba0c8fa4d3ab48c2f07bb6eab83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6d10126ecd8e43c4be054d0977f9d78d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0ecf68fa3311440797cdc3543d5c3cbe",
              "IPY_MODEL_323389e65a52469ab4217929cac4f7ad"
            ]
          }
        },
        "6d10126ecd8e43c4be054d0977f9d78d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ecf68fa3311440797cdc3543d5c3cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_553acc9be7d84a73944a1f29fd69da4b",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1520013706,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1520013706,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8d924d58c1d34a6bb445b67e070df042"
          }
        },
        "323389e65a52469ab4217929cac4f7ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7b9d36ee0764443f9e1a9b6da6ab42f1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 1.52G/1.52G [00:27&lt;00:00, 54.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_489abbf700f942929e21dbf97eff0c4e"
          }
        },
        "553acc9be7d84a73944a1f29fd69da4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8d924d58c1d34a6bb445b67e070df042": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7b9d36ee0764443f9e1a9b6da6ab42f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "489abbf700f942929e21dbf97eff0c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1f9df3aab26d40ac9cff9359546fa30a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_17f6224bafcb4d789b831ece60188bc8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0831f963251f4fef922ce0eff014babc",
              "IPY_MODEL_9d56503eaace42428dab014570fb46d2"
            ]
          }
        },
        "17f6224bafcb4d789b831ece60188bc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0831f963251f4fef922ce0eff014babc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2e59b9c4599341298b95f9dc457394fe",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78dc39a0a3d446e085d6505e6f306001"
          }
        },
        "9d56503eaace42428dab014570fb46d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2be2a1b8c9e841b991cb824d19bbc0f7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 1.04M/1.04M [00:00&lt;00:00, 6.00MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9d29b44c7e44d079900fe5f05f1e553"
          }
        },
        "2e59b9c4599341298b95f9dc457394fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78dc39a0a3d446e085d6505e6f306001": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2be2a1b8c9e841b991cb824d19bbc0f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b9d29b44c7e44d079900fe5f05f1e553": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d8589186a82b4a75ae37217ca804c39c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_66e68150ae60499b82344151f6462c63",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9c28b17626f24817b89350fb7595d9fa",
              "IPY_MODEL_bef601f75c8b473eb6aee7943d0497d1"
            ]
          }
        },
        "66e68150ae60499b82344151f6462c63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c28b17626f24817b89350fb7595d9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fe2c8c937bd5426c98b5109923975736",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b2f14080a31b46cf96bc2d5b5a2df66e"
          }
        },
        "bef601f75c8b473eb6aee7943d0497d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ac1ecb4df5f1424db8dfafe0f8eead66",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 456k/456k [00:00&lt;00:00, 3.07MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6eca84f7e0864a95aa75c83830adddcd"
          }
        },
        "fe2c8c937bd5426c98b5109923975736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b2f14080a31b46cf96bc2d5b5a2df66e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ac1ecb4df5f1424db8dfafe0f8eead66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6eca84f7e0864a95aa75c83830adddcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunwilliamyu/interactive-fiction-class.github.io/blob/master/homeworks/language-model/hw4_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK_lPvRlBdMB",
        "colab_type": "text"
      },
      "source": [
        "# Homework 4 - Finetune GPT-2\n",
        "This colab demonstrates how to fine-tune GPT-2 on a dataset of presidential speeches. We use the [Hugging Face Transformer](https://github.com/huggingface/transformers) library in order to do this.\n",
        "\n",
        "**IMPORTANT: Make sure that you have GPU set as your Hardware Accelerator in `Runtime > Change runtime type` before running this Colab.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biV1z0koaDHT",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGscdaCtpmbV",
        "colab_type": "text"
      },
      "source": [
        "### Install HuggingFace Transfomers library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uicio9FLPv5j",
        "colab_type": "code",
        "outputId": "ddba7294-03d3-4219-b18e-72e874c365be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "\n",
        "import os\n",
        "os.chdir('/content/transformers')\n",
        "\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt\n",
        "\n",
        "os.chdir('/content/transformers/examples')\n",
        "\n",
        "!pip install dict_to_obj\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 19942 (delta 17), reused 24 (delta 7), pack-reused 19901\u001b[K\n",
            "Receiving objects: 100% (19942/19942), 11.99 MiB | 7.51 MiB/s, done.\n",
            "Resolving deltas: 100% (14479/14479), done.\n",
            "Processing /content/transformers\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (1.17.5)\n",
            "Collecting tokenizers==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/1d/ea7e2c628942e686595736f73678348272120d026b7acd54fe43e5211bb1/tokenizers-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (1.11.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0MB 34.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 870kB 27.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5.0) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5.0) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5.0) (0.3.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.0) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.0) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.5.0) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.5.0) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.5.0) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.5.0) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.5.0) (0.15.2)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.5.0-cp36-none-any.whl size=497322 sha256=37cc1bc79ca37967939a72fec34e976ef45d817c7803cd62d15af696352e646c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o4jia_nu/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=ccfe0fb85df38706cfb17eca0d485757627d37f058e0a827744edbe762ddd536\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.0 transformers-2.5.0\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 3)) (0.22.1)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.17.5)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (45.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.34.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->-r ./examples/requirements.txt (line 4)) (2.2.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=dba5588369b13a455664fb75fcc732b6d384d7b7aca60c2e76cb820edea11f3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: tensorboardX, seqeval\n",
            "Successfully installed seqeval-0.0.12 tensorboardX-2.0\n",
            "Collecting dict_to_obj\n",
            "  Downloading https://files.pythonhosted.org/packages/71/84/95cb71e10a1627076f6e2ba6cd81683e4ba344a885aa2d8b38a9a33f53c4/dict_to_obj-0.0.2.tar.gz\n",
            "Building wheels for collected packages: dict-to-obj\n",
            "  Building wheel for dict-to-obj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dict-to-obj: filename=dict_to_obj-0.0.2-cp36-none-any.whl size=1444 sha256=3a729d7e9aa2173d4f6994f17fe698caad51ad29e619ed39e646687933fe4b1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/71/a8/727ff01010936e40001af4c79983f505aecb5f271faa91c3e3\n",
            "Successfully built dict-to-obj\n",
            "Installing collected packages: dict-to-obj\n",
            "Successfully installed dict-to-obj-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weh0BoPfk1zc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "aab92594-933f-4570-baad-b6ff95e73f47"
      },
      "source": [
        "import torch\n",
        "import run_language_modeling\n",
        "import run_generation\n",
        "from dict_to_obj import DictToObj\n",
        "import collections\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX2LEl-uwMXp",
        "colab_type": "text"
      },
      "source": [
        "### Mount your Google Drive\n",
        "We will be saving trained checkpoints on your Google Drive so that they can be accessed even if the Colab session dies. Make sure to login with your UPenn credentials, as you will be saving several gigabytes of data, and Penn gives you unlimited Drive storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ6FFHiMMP0V",
        "colab_type": "code",
        "outputId": "42df2a36-5613-4c61-a104-25c09fd5a37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEuEqWOhwn-a",
        "colab_type": "text"
      },
      "source": [
        "### Download presidential speech data.\n",
        "It's already been split into a train, valid, and test set for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GzNXh7ap_R3",
        "colab_type": "code",
        "outputId": "48fa16b0-f98b-4d62-ec18-16ce32b0ae04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "source": [
        "# Download the train and test set.\n",
        "!wget -nc -O /content/presidential_speeches_test.txt https://raw.githubusercontent.com/interactive-fiction-class/interactive-fiction-class.github.io/master/homeworks/language-model/presidential_speeches_test.txt\n",
        "!wget -nc -O /content/presidential_speeches_valid.txt https://raw.githubusercontent.com/interactive-fiction-class/interactive-fiction-class.github.io/master/homeworks/language-model/presidential_speeches_valid.txt\n",
        "!wget -nc -O /content/presidential_speeches_train.txt https://raw.githubusercontent.com/interactive-fiction-class/interactive-fiction-class.github.io/master/homeworks/language-model/presidential_speeches_train.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-22 07:26:11--  https://raw.githubusercontent.com/interactive-fiction-class/interactive-fiction-class.github.io/master/homeworks/language-model/presidential_speeches_test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 622537 (608K) [text/plain]\n",
            "Saving to: â€˜/content/presidential_speeches_test.txtâ€™\n",
            "\n",
            "/content/presidenti 100%[===================>] 607.95K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-02-22 07:26:13 (9.54 MB/s) - â€˜/content/presidential_speeches_test.txtâ€™ saved [622537/622537]\n",
            "\n",
            "--2020-02-22 07:26:14--  https://raw.githubusercontent.com/interactive-fiction-class/interactive-fiction-class.github.io/master/homeworks/language-model/presidential_speeches_valid.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1117165 (1.1M) [text/plain]\n",
            "Saving to: â€˜/content/presidential_speeches_valid.txtâ€™\n",
            "\n",
            "/content/presidenti 100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-02-22 07:26:15 (14.3 MB/s) - â€˜/content/presidential_speeches_valid.txtâ€™ saved [1117165/1117165]\n",
            "\n",
            "--2020-02-22 07:26:16--  https://raw.githubusercontent.com/interactive-fiction-class/interactive-fiction-class.github.io/master/homeworks/language-model/presidential_speeches_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20269025 (19M) [text/plain]\n",
            "Saving to: â€˜/content/presidential_speeches_train.txtâ€™\n",
            "\n",
            "/content/presidenti 100%[===================>]  19.33M  29.7MB/s    in 0.7s    \n",
            "\n",
            "2020-02-22 07:26:19 (29.7 MB/s) - â€˜/content/presidential_speeches_train.txtâ€™ saved [20269025/20269025]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VzV8iTrphJl",
        "colab_type": "text"
      },
      "source": [
        "## Finetune and Eval\n",
        "The Hugging Face library provides a script [run_language_modeling.py](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py) which contains all of the code for training and evaluating a language model.\n",
        "\n",
        "We will be calling this script directly from the command line in order to launch training. We will also use functions from this script to conduct evaluation and generate samples at inference time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOzFhwDSqOg3",
        "colab_type": "text"
      },
      "source": [
        "### Launch fine-tuninng\n",
        "We will be calling `run_language_modeling.py` from the command line to launch fine-tuning, **Running fine-tuning may take several hours.** Every `save_steps` steps, a checkpoint is saved to disk. The checkpoint contains all the learned weights for your model, and you can  always reload the model from a saved checkpoint, even if your Colab has crashed.\n",
        "\n",
        "Below is an explanation of some of the arguments you might want to modify in the command below. \n",
        "\n",
        "* `--line_by_line`: Add `--line_by_line` if distinct lines of the text should be treated as distinct training examples. For example, if your dataset contains one story/tweet/article per line, this should be set.\n",
        "* `--num_train_epochs`: The number of times to iterate over the train set. Increasing the number of epochs may result in better performance, but making this number too high will cause the model to overfit on the train set.\n",
        "* `--block_size`: Your training text is truncated into blocks of this length. At test time, you will only want to generate sequences that are at most this length.\n",
        "* `--gradient_accumulation_steps`: Update the model weights every this many steps. You shold set this to >1 when the batch size is very small to improve training stability.\n",
        "* `--output_dir`: This is the where checkpoints will get saved. When you finetune on your own dataset, you should change this path. We recommend saving checkpoints to your Google Drive (`/content/drive/My Drive/`) so you can access them even if the Colab session dies.\n",
        "* `--model_name_or_path` The path to the model weights to use when starting fine-tuning. You can set this to `gpt2-medium` to initialize with GPT-2's 355 million parameter model, or `gpt2` to initialize with their smaller 124 million parameter model. You can also set this to one of your own checkpoints to restart your training job if it crashes.\n",
        "\n",
        "**I am getting out-of memory errors. What do I do?**\n",
        "\n",
        "The number of trainable paramters in the model is a function of the `block_size` and the `batch_size`. If you are getting out-of-memory errors, then try drecreasing these value.\n",
        "\n",
        "**Oh no! My computer went to sleep and the Colab disconnected.**\n",
        "\n",
        "The train job might have still completed. Check the `output_dir` in your Google Drive to see if checkpoint files have been created there.\n",
        "\n",
        "**Training is taking foreverrrrrr.**\n",
        "\n",
        "Try decreasing `num_train_epochs` or changing `model_name_or_path` to `gpt2` instead of `gpt2-medium`.\n",
        "If your evaluation set is very large, you might also want to remove the `evaluate_during_training` flag or increase `logging_steps`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C33GutF1QVEV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "28774b52-2f95-41eb-d763-81d1299a5a2a"
      },
      "source": [
        "!python run_language_modeling.py \\\n",
        "    --output_dir='/content/drive/My Drive/finetuned_models/presidential_speeches3' \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2-medium \\\n",
        "    --save_total_limit=5 \\\n",
        "    --num_train_epochs=1.0 \\\n",
        "    --do_train \\\n",
        "    --evaluate_during_training \\\n",
        "    --logging_steps=5 \\\n",
        "    --save_steps=5 \\\n",
        "    --train_data_file=/content/presidential_speeches_train.txt \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=/content/presidential_speeches_valid.txt \\\n",
        "    --per_gpu_train_batch_size=2 \\\n",
        "    --per_gpu_eval_batch_size=2 \\\n",
        "    --block_size=128 \\\n",
        "    --gradient_accumulation_steps=5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/22/2020 07:52:28 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "02/22/2020 07:52:28 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-config.json from cache at /root/.cache/torch/transformers/98aa65385e18b0efd17acd8bf64dcdf21406bb0c99c801c2d3c9f6bfd1f48f29.266bb9683aedfcb1f7006ad2e6894fce82b8dbbae8125f4fc8570b818005b83d\n",
            "02/22/2020 07:52:28 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/22/2020 07:52:30 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json from cache at /root/.cache/torch/transformers/f20f05d3ae37c4e3cd56764d48e566ea5adeba153dcee6eb82a18822c9c731ec.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "02/22/2020 07:52:30 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-merges.txt from cache at /root/.cache/torch/transformers/6d882670c55563617571fe0c97df88626fb5033927b40fc18a8acf98dafd4946.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "02/22/2020 07:52:31 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-pytorch_model.bin from cache at /root/.cache/torch/transformers/4b337a4f3b7d3e1518f799e238af607498c02938a3390152aaec7d4dabca5a02.8769029be4f66a5ae1055eefdd1d11621b901d510654266b8681719fff492d6e\n",
            "02/22/2020 07:53:17 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=128, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/presidential_speeches_valid.txt', evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=5, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=5, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='gpt2-medium', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/content/drive/My Drive/finetuned_models/presidential_speeches3', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=2, per_gpu_train_batch_size=2, save_steps=5, save_total_limit=5, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='/content/presidential_speeches_train.txt', warmup_steps=0, weight_decay=0.0)\n",
            "02/22/2020 07:53:17 - INFO - __main__ -   Loading features from cached file /content/gpt2_cached_lm_128_presidential_speeches_train.txt\n",
            "02/22/2020 07:53:19 - INFO - __main__ -   ***** Running training *****\n",
            "02/22/2020 07:53:19 - INFO - __main__ -     Num examples = 31671\n",
            "02/22/2020 07:53:19 - INFO - __main__ -     Num Epochs = 1\n",
            "02/22/2020 07:53:19 - INFO - __main__ -     Instantaneous batch size per GPU = 2\n",
            "02/22/2020 07:53:19 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 10\n",
            "02/22/2020 07:53:19 - INFO - __main__ -     Gradient Accumulation steps = 5\n",
            "02/22/2020 07:53:19 - INFO - __main__ -     Total optimization steps = 3167\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/15836 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/15836 [00:00<49:28,  5.33it/s]\u001b[A\n",
            "Iteration:   0% 2/15836 [00:00<46:26,  5.68it/s]\u001b[A\n",
            "Iteration:   0% 3/15836 [00:00<44:07,  5.98it/s]\u001b[A\n",
            "Iteration:   0% 4/15836 [00:00<42:37,  6.19it/s]\u001b[A\n",
            "Iteration:   0% 5/15836 [00:00<51:26,  5.13it/s]\u001b[A\n",
            "Iteration:   0% 6/15836 [00:01<47:38,  5.54it/s]\u001b[A\n",
            "Iteration:   0% 7/15836 [00:01<44:37,  5.91it/s]\u001b[A\n",
            "Iteration:   0% 8/15836 [00:01<42:49,  6.16it/s]\u001b[A\n",
            "Iteration:   0% 9/15836 [00:01<40:53,  6.45it/s]\u001b[A\n",
            "Iteration:   0% 10/15836 [00:01<46:58,  5.62it/s]\u001b[A\n",
            "Iteration:   0% 11/15836 [00:01<43:52,  6.01it/s]\u001b[A\n",
            "Iteration:   0% 12/15836 [00:01<41:25,  6.37it/s]\u001b[A\n",
            "Iteration:   0% 13/15836 [00:02<39:39,  6.65it/s]\u001b[A\n",
            "Iteration:   0% 14/15836 [00:02<38:38,  6.83it/s]\u001b[A\n",
            "Iteration:   0% 15/15836 [00:02<45:35,  5.78it/s]\u001b[A\n",
            "Iteration:   0% 16/15836 [00:02<42:44,  6.17it/s]\u001b[A\n",
            "Iteration:   0% 17/15836 [00:02<40:25,  6.52it/s]\u001b[A\n",
            "Iteration:   0% 18/15836 [00:02<39:43,  6.64it/s]\u001b[A\n",
            "Iteration:   0% 19/15836 [00:03<38:23,  6.87it/s]\u001b[A\n",
            "Iteration:   0% 20/15836 [00:03<45:10,  5.84it/s]\u001b[A\n",
            "Iteration:   0% 21/15836 [00:03<42:27,  6.21it/s]\u001b[A\n",
            "Iteration:   0% 22/15836 [00:03<40:42,  6.48it/s]\u001b[A\n",
            "Iteration:   0% 23/15836 [00:03<39:11,  6.72it/s]\u001b[A\n",
            "Iteration:   0% 24/15836 [00:03<39:00,  6.76it/s]\u001b[A02/22/2020 07:53:23 - INFO - __main__ -   Loading features from cached file /content/gpt2_cached_lm_128_presidential_speeches_valid.txt\n",
            "02/22/2020 07:53:23 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "02/22/2020 07:53:23 - INFO - __main__ -     Num examples = 1741\n",
            "02/22/2020 07:53:23 - INFO - __main__ -     Batch size = 2\n",
            "\n",
            "\n",
            "Evaluating:   0% 0/871 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   0% 2/871 [00:00<00:45, 19.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   1% 5/871 [00:00<00:42, 20.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   1% 8/871 [00:00<00:40, 21.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   1% 11/871 [00:00<00:38, 22.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   2% 14/871 [00:00<00:38, 22.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   2% 17/871 [00:00<00:37, 23.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   2% 20/871 [00:00<00:39, 21.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   3% 23/871 [00:01<00:40, 20.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   3% 25/871 [00:01<00:42, 19.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   3% 28/871 [00:01<00:42, 19.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   4% 31/871 [00:01<00:40, 20.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   4% 34/871 [00:01<00:38, 21.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   4% 37/871 [00:01<00:37, 22.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   5% 40/871 [00:01<00:36, 22.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   5% 43/871 [00:01<00:37, 22.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   5% 46/871 [00:02<00:38, 21.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   6% 49/871 [00:02<00:39, 20.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   6% 52/871 [00:02<00:40, 20.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   6% 55/871 [00:02<00:41, 19.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   7% 58/871 [00:02<00:40, 20.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   7% 61/871 [00:02<00:40, 19.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   7% 64/871 [00:03<00:40, 20.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   8% 67/871 [00:03<00:40, 19.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   8% 70/871 [00:03<00:40, 19.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   8% 73/871 [00:03<00:38, 20.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   9% 76/871 [00:03<00:37, 21.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   9% 79/871 [00:03<00:35, 22.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   9% 82/871 [00:03<00:34, 22.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  10% 85/871 [00:03<00:35, 22.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  10% 88/871 [00:04<00:34, 22.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  10% 91/871 [00:04<00:34, 22.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  11% 94/871 [00:04<00:34, 22.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  11% 97/871 [00:04<00:33, 23.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  11% 100/871 [00:04<00:33, 23.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  12% 103/871 [00:04<00:32, 23.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  12% 106/871 [00:04<00:32, 23.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  13% 109/871 [00:05<00:34, 22.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  13% 112/871 [00:05<00:33, 22.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  13% 115/871 [00:05<00:33, 22.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  14% 118/871 [00:05<00:33, 22.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  14% 121/871 [00:05<00:33, 22.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  14% 124/871 [00:05<00:32, 22.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  15% 127/871 [00:05<00:32, 23.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  15% 130/871 [00:05<00:31, 23.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  15% 133/871 [00:06<00:31, 23.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  16% 136/871 [00:06<00:31, 23.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  16% 139/871 [00:06<00:31, 23.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  16% 142/871 [00:06<00:30, 23.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  17% 145/871 [00:06<00:31, 23.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  17% 148/871 [00:06<00:32, 22.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  17% 151/871 [00:06<00:33, 21.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  18% 154/871 [00:07<00:34, 20.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  18% 157/871 [00:07<00:34, 20.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  18% 160/871 [00:07<00:34, 20.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  19% 163/871 [00:07<00:33, 21.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  19% 166/871 [00:07<00:33, 21.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  19% 169/871 [00:07<00:33, 20.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  20% 172/871 [00:07<00:34, 20.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  20% 175/871 [00:08<00:34, 20.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  20% 178/871 [00:08<00:33, 21.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  21% 181/871 [00:08<00:33, 20.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  21% 184/871 [00:08<00:33, 20.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  21% 187/871 [00:08<00:33, 20.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  22% 190/871 [00:08<00:34, 19.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  22% 193/871 [00:08<00:34, 19.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  22% 195/871 [00:09<00:34, 19.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  23% 197/871 [00:09<00:33, 19.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  23% 200/871 [00:09<00:33, 20.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  23% 203/871 [00:09<00:32, 20.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  24% 206/871 [00:09<00:30, 21.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  24% 209/871 [00:09<00:29, 22.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  24% 212/871 [00:09<00:29, 22.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  25% 215/871 [00:09<00:29, 22.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  25% 218/871 [00:10<00:28, 22.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  25% 221/871 [00:10<00:28, 23.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  26% 224/871 [00:10<00:28, 22.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  26% 227/871 [00:10<00:29, 22.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  26% 230/871 [00:10<00:28, 22.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  27% 233/871 [00:10<00:27, 23.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  27% 236/871 [00:10<00:28, 22.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  27% 239/871 [00:11<00:29, 21.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  28% 242/871 [00:11<00:30, 20.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  28% 245/871 [00:11<00:29, 21.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  28% 248/871 [00:11<00:28, 21.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  29% 251/871 [00:11<00:28, 22.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  29% 254/871 [00:11<00:27, 22.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  30% 257/871 [00:11<00:26, 23.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  30% 260/871 [00:11<00:26, 23.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  30% 263/871 [00:12<00:25, 23.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  31% 266/871 [00:12<00:25, 23.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  31% 269/871 [00:12<00:26, 23.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  31% 272/871 [00:12<00:26, 22.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  32% 275/871 [00:12<00:25, 23.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  32% 278/871 [00:12<00:25, 23.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  32% 281/871 [00:12<00:24, 23.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  33% 284/871 [00:12<00:25, 22.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  33% 287/871 [00:13<00:26, 21.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  33% 290/871 [00:13<00:26, 21.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  34% 293/871 [00:13<00:26, 21.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  34% 296/871 [00:13<00:27, 21.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  34% 299/871 [00:13<00:27, 20.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  35% 302/871 [00:13<00:27, 20.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  35% 305/871 [00:14<00:26, 21.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  35% 308/871 [00:14<00:26, 20.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  36% 311/871 [00:14<00:27, 20.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  36% 314/871 [00:14<00:27, 20.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  36% 317/871 [00:14<00:27, 19.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  37% 320/871 [00:14<00:27, 19.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  37% 323/871 [00:14<00:27, 19.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  37% 326/871 [00:15<00:26, 20.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  38% 329/871 [00:15<00:25, 21.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  38% 332/871 [00:15<00:24, 22.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  38% 335/871 [00:15<00:24, 22.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  39% 338/871 [00:15<00:24, 21.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  39% 341/871 [00:15<00:24, 21.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  39% 344/871 [00:15<00:25, 20.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  40% 347/871 [00:16<00:24, 21.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  40% 350/871 [00:16<00:23, 21.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  41% 353/871 [00:16<00:22, 22.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  41% 356/871 [00:16<00:23, 22.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  41% 359/871 [00:16<00:22, 22.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  42% 362/871 [00:16<00:22, 22.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  42% 365/871 [00:16<00:21, 23.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  42% 368/871 [00:16<00:21, 23.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  43% 371/871 [00:17<00:21, 23.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  43% 374/871 [00:17<00:20, 23.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  43% 377/871 [00:17<00:20, 24.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  44% 380/871 [00:17<00:20, 23.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  44% 383/871 [00:17<00:20, 23.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  44% 386/871 [00:17<00:21, 23.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  45% 389/871 [00:17<00:21, 22.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  45% 392/871 [00:17<00:21, 22.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  45% 395/871 [00:18<00:20, 22.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  46% 398/871 [00:18<00:20, 22.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  46% 401/871 [00:18<00:20, 23.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  46% 404/871 [00:18<00:19, 23.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  47% 407/871 [00:18<00:19, 23.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  47% 410/871 [00:18<00:19, 23.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  47% 413/871 [00:18<00:19, 23.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  48% 416/871 [00:18<00:19, 23.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  48% 419/871 [00:19<00:19, 23.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  48% 422/871 [00:19<00:19, 23.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  49% 425/871 [00:19<00:19, 22.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  49% 428/871 [00:19<00:19, 23.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  49% 431/871 [00:19<00:19, 23.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  50% 434/871 [00:19<00:18, 23.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  50% 437/871 [00:19<00:18, 23.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  51% 440/871 [00:19<00:18, 23.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  51% 443/871 [00:20<00:17, 23.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  51% 446/871 [00:20<00:17, 23.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  52% 449/871 [00:20<00:17, 23.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  52% 452/871 [00:20<00:17, 23.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  52% 455/871 [00:20<00:17, 23.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  53% 458/871 [00:20<00:17, 23.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  53% 461/871 [00:20<00:17, 22.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  53% 464/871 [00:21<00:18, 22.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54% 467/871 [00:21<00:17, 23.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54% 470/871 [00:21<00:17, 23.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54% 473/871 [00:21<00:16, 23.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55% 476/871 [00:21<00:16, 23.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55% 479/871 [00:21<00:16, 23.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55% 482/871 [00:21<00:16, 23.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56% 485/871 [00:21<00:16, 23.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56% 488/871 [00:22<00:17, 22.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56% 491/871 [00:22<00:17, 21.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57% 494/871 [00:22<00:17, 21.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57% 497/871 [00:22<00:18, 20.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57% 500/871 [00:22<00:18, 19.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58% 503/871 [00:22<00:18, 19.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58% 506/871 [00:22<00:17, 20.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58% 509/871 [00:23<00:17, 20.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59% 512/871 [00:23<00:17, 20.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59% 515/871 [00:23<00:16, 21.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59% 518/871 [00:23<00:15, 22.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60% 521/871 [00:23<00:15, 22.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60% 524/871 [00:23<00:15, 23.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61% 527/871 [00:23<00:14, 23.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61% 530/871 [00:24<00:14, 23.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61% 533/871 [00:24<00:14, 23.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62% 536/871 [00:24<00:14, 23.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62% 539/871 [00:24<00:13, 23.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62% 542/871 [00:24<00:13, 24.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63% 545/871 [00:24<00:14, 22.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63% 548/871 [00:24<00:14, 21.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63% 551/871 [00:24<00:15, 21.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64% 554/871 [00:25<00:14, 21.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64% 557/871 [00:25<00:14, 21.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64% 560/871 [00:25<00:13, 22.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65% 563/871 [00:25<00:13, 22.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65% 566/871 [00:25<00:14, 21.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65% 569/871 [00:25<00:15, 20.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66% 572/871 [00:25<00:14, 20.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66% 575/871 [00:26<00:14, 20.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66% 578/871 [00:26<00:13, 21.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67% 581/871 [00:26<00:12, 22.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67% 584/871 [00:26<00:12, 22.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67% 587/871 [00:26<00:12, 23.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68% 590/871 [00:26<00:12, 22.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68% 593/871 [00:26<00:12, 22.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68% 596/871 [00:26<00:11, 23.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69% 599/871 [00:27<00:11, 23.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69% 602/871 [00:27<00:11, 23.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69% 605/871 [00:27<00:11, 23.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70% 608/871 [00:27<00:11, 23.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70% 611/871 [00:27<00:10, 23.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70% 614/871 [00:27<00:11, 22.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71% 617/871 [00:27<00:11, 21.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71% 620/871 [00:28<00:11, 21.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72% 623/871 [00:28<00:11, 21.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72% 626/871 [00:28<00:11, 22.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72% 629/871 [00:28<00:10, 22.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73% 632/871 [00:28<00:10, 22.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73% 635/871 [00:28<00:10, 22.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73% 638/871 [00:28<00:10, 21.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74% 641/871 [00:29<00:11, 20.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74% 644/871 [00:29<00:11, 20.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74% 647/871 [00:29<00:11, 19.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75% 650/871 [00:29<00:10, 20.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75% 653/871 [00:29<00:10, 21.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75% 656/871 [00:29<00:09, 22.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76% 659/871 [00:29<00:09, 22.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76% 662/871 [00:29<00:09, 22.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76% 665/871 [00:30<00:08, 23.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77% 668/871 [00:30<00:08, 23.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77% 671/871 [00:30<00:08, 22.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77% 674/871 [00:30<00:09, 21.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78% 677/871 [00:30<00:09, 21.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78% 680/871 [00:30<00:09, 20.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78% 683/871 [00:30<00:09, 20.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79% 686/871 [00:31<00:09, 20.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79% 689/871 [00:31<00:08, 20.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79% 692/871 [00:31<00:08, 21.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80% 695/871 [00:31<00:08, 21.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80% 698/871 [00:31<00:07, 22.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80% 701/871 [00:31<00:07, 22.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81% 704/871 [00:31<00:07, 22.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81% 707/871 [00:32<00:07, 22.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82% 710/871 [00:32<00:06, 23.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82% 713/871 [00:32<00:06, 23.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82% 716/871 [00:32<00:06, 23.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83% 719/871 [00:32<00:06, 23.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83% 722/871 [00:32<00:06, 23.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83% 725/871 [00:32<00:06, 23.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84% 728/871 [00:32<00:06, 23.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84% 731/871 [00:33<00:06, 22.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84% 734/871 [00:33<00:06, 21.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85% 737/871 [00:33<00:06, 21.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85% 740/871 [00:33<00:05, 21.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85% 743/871 [00:33<00:06, 21.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86% 746/871 [00:33<00:06, 20.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86% 749/871 [00:33<00:05, 20.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86% 752/871 [00:34<00:05, 21.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87% 755/871 [00:34<00:05, 20.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87% 758/871 [00:34<00:05, 20.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87% 761/871 [00:34<00:05, 21.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88% 764/871 [00:34<00:04, 22.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88% 767/871 [00:34<00:04, 21.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88% 770/871 [00:34<00:04, 21.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89% 773/871 [00:35<00:04, 22.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89% 776/871 [00:35<00:04, 22.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89% 779/871 [00:35<00:04, 22.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90% 782/871 [00:35<00:03, 22.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90% 785/871 [00:35<00:03, 22.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90% 788/871 [00:35<00:03, 21.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91% 791/871 [00:35<00:03, 21.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91% 794/871 [00:35<00:03, 21.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92% 797/871 [00:36<00:03, 22.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92% 800/871 [00:36<00:03, 21.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92% 803/871 [00:36<00:03, 21.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93% 806/871 [00:36<00:03, 20.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93% 809/871 [00:36<00:03, 20.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93% 812/871 [00:36<00:02, 21.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94% 815/871 [00:36<00:02, 21.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94% 818/871 [00:37<00:02, 22.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94% 821/871 [00:37<00:02, 22.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95% 824/871 [00:37<00:02, 22.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95% 827/871 [00:37<00:01, 23.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95% 830/871 [00:37<00:01, 22.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96% 833/871 [00:37<00:01, 21.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96% 836/871 [00:37<00:01, 21.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96% 839/871 [00:38<00:01, 20.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97% 842/871 [00:38<00:01, 20.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97% 845/871 [00:38<00:01, 20.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97% 848/871 [00:38<00:01, 21.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98% 851/871 [00:38<00:00, 21.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98% 854/871 [00:38<00:00, 22.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98% 857/871 [00:38<00:00, 22.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99% 860/871 [00:39<00:00, 22.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99% 863/871 [00:39<00:00, 23.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99% 866/871 [00:39<00:00, 23.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100% 869/871 [00:39<00:00, 22.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100% 871/871 [00:39<00:00, 22.05it/s]\u001b[A\u001b[A02/22/2020 07:54:03 - INFO - __main__ -   ***** Eval results  *****\n",
            "02/22/2020 07:54:03 - INFO - __main__ -     perplexity = tensor(25.2938)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "02/22/2020 07:54:03 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/finetuned_models/presidential_speeches3/checkpoint-5/config.json\n",
            "02/22/2020 07:54:14 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/finetuned_models/presidential_speeches3/checkpoint-5/pytorch_model.bin\n",
            "02/22/2020 07:54:15 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/finetuned_models/presidential_speeches3/checkpoint-5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9mAiosB2wBm",
        "colab_type": "text"
      },
      "source": [
        "### Compute perplexity of a dataset.\n",
        "This section shows how to compute perplexity of a dataset according to either the pre-trained or your fine-tuned language model. While this is possible to do by calling `run_language_modeling.py` on the command-line as above, we'll instead call the Python functions directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiqSgGhtcDNd",
        "colab_type": "text"
      },
      "source": [
        "#### Look at what checkpoints are available\n",
        "Run `ls` to look at what checkpoints saved been saved. You'll want to set `CHECKPOINT_PATH` below to one of these in order to evaluate the model weights saved in that checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk_qHytBIETo",
        "colab_type": "code",
        "outputId": "3f67d2aa-5f36-4634-e4ab-8f1d400fccaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!ls '/content/drive/My Drive/finetuned_model/presidential_speeches'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/drive/My Drive/finetuned_model/presidential_speeches': No such file or directory\n",
            " 2017May-CSHL-BoG-reimburse.gsheet\n",
            " 2018-10-06-Adam-papergame-demo-playtest.gdoc\n",
            " 2019-02-18-levenshtein-review-response.gdoc\n",
            " ADI_Summer_Offsite_2015-2015-08-28.zip\n",
            "'Advanced linear algebra.gdoc'\n",
            " AmericanMythos.gsheet\n",
            "'Amia 2018 notes.gdoc'\n",
            " Applications\n",
            " ar-sketching-review.gdoc\n",
            "'Arts Scholars Share Decoration planning.gdoc'\n",
            " backup-2016-05-25\n",
            "'Beauty and the Beast-Belle-SheetMusicDownload.pdf'\n",
            " calls-20180619105128.xml\n",
            " Chrombook-Downloads-20150115\n",
            "'Colab Notebooks'\n",
            " combined2.csv\n",
            " ComMIT\n",
            "'ComMIT (1)'\n",
            "'CORA 2 - Design Doc.gdoc'\n",
            "'Dan Interview 2016-01-24.gdoc'\n",
            "'Daphne Google Draft.gdoc'\n",
            "'Daphne Ippolito - Teams.html'\n",
            "'Daphne Ippolito - Teams.mhtml'\n",
            "'Daphne notes.gdoc'\n",
            " DaphneScratch\n",
            "'Diana Wedding speech.gdoc'\n",
            " documentation\n",
            " encfs-small-2018-12-25.tar.bz2\n",
            "'FB Election 2016.gdoc'\n",
            " FLjtaEmUsoi.png\n",
            " Genome2017_AbstractBook.pdf\n",
            "'Gmail table.gdoc'\n",
            " Google_Employment_Application_Yun_William_Yu.pdf\n",
            "'Google Photos'\n",
            " grants\n",
            " Gymnastics\n",
            "'Gymnastics is hard.gdoc'\n",
            "'Hail Notes.gdoc'\n",
            "'Harvard thoughts.gsheet'\n",
            " headshot_dal_square.jpg\n",
            "'Hertz SW 2017 Agenda 7-18-17.pdf'\n",
            " House\n",
            " idle-saves\n",
            "'Imposter Syndrome Note.gdoc'\n",
            "'India Expenses.gsheet'\n",
            " Inventory.gsheet\n",
            " ISMB2018-reimbursement.gsheet\n",
            "'Lassen Backpacking Trip.gsheet'\n",
            "'Life Cinema Films Watched.gsheet'\n",
            " LREV-D-19-00201_reviewer.pdf\n",
            " MIT-classes\n",
            "'Mystery Hunt 2015'\n",
            "'Mystery Hunt 2016'\n",
            "'NewRGBComputer - Bifrost.gdoc'\n",
            "'Nicohas Cage Story Idea.gdoc'\n",
            "'nothing to see here.gdoc'\n",
            " Paris_Iceland_2018.gsheet\n",
            " PartialFacultyList.gsheet\n",
            "'Ping pong paddles.gdoc'\n",
            " Powerpoint_test\n",
            "'Presentation notes.gdoc'\n",
            " privacy-paper\n",
            "'Project Ideas - Research.gdoc'\n",
            " refs\n",
            "'Rental map.gmap'\n",
            " research\n",
            "'Research Ideas.gdoc'\n",
            "'Screenshot 2015-08-08 at 4.56.03 PM.png'\n",
            "'Screenshot 2015-08-08 at 4.56.18 PM.png'\n",
            "'Screenshot 2015-08-08 at 4.56.36 PM.png'\n",
            "'Screenshot 2017-01-10 at 6.08.15 PM.png'\n",
            " SDFCU.gdoc\n",
            "'Search Paper w  Mike Waterman.gdoc'\n",
            "'Seckel St Furniture.gdoc'\n",
            " sms-20160105132329.xml\n",
            " sms-20180619105128.xml\n",
            " sms-onedrive-2011.txt\n",
            " sms.xsl\n",
            " snowfall.gif\n",
            " snowfall_globe_166mag.gif\n",
            "'Specific Aims102416_Clemons.gdoc'\n",
            "'Stanislaus Backpacking Trip 2016-July-04.gsheet'\n",
            "'STOC-HyperMinHash reviews.gdoc'\n",
            " Switzerland-Italy-2019.gsheet\n",
            " TEDx-Workshop-Notes-Dishita.gdoc\n",
            "'Thanksgiving Recipes.gdoc'\n",
            "'Toronto notes.gdoc'\n",
            " transfer\n",
            " Travel\n",
            "'Trip expense notes.gsheet'\n",
            "'University of Toronto personal notes.gdoc'\n",
            " Untitled\n",
            "'Untitled document.gdoc'\n",
            "'Untitled spreadsheet (1).gsheet'\n",
            "'Untitled spreadsheet (2).gsheet'\n",
            "'Untitled spreadsheet.gsheet'\n",
            "'UofT offer.gdoc'\n",
            " winter-2019-ideapad\n",
            " XComiCloudsave0\n",
            "'Yosemite Trip (Jonas, Davie, Daphne, Alane, William).gsheet'\n",
            "'Yu - Hertz Gates Update 2017-11-10.gdoc'\n",
            "'Yu UTSC.pdf'\n",
            " ywy-presentation-SCSNW-NPOPB.pdf\n",
            "'Ziye TAo.gdoc'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRFwV1Ry3Evk",
        "colab_type": "text"
      },
      "source": [
        "#### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc2VCFBG3pFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(args):\n",
        "  \"\"\"Creates a model and loads in weights for it.\"\"\"\n",
        "  config_class, model_class, _ = run_language_modeling.MODEL_CLASSES[args.model_type]\n",
        "  config = config_class.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  model = model_class.from_pretrained(\n",
        "      args.model_name_or_path,\n",
        "      from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "      config=config,\n",
        "      cache_dir=None,\n",
        "  )\n",
        "  model.to(args.device)\n",
        "  return model\n",
        "\n",
        "def set_seed(seed):\n",
        "  \"\"\"Set the random seed.\"\"\"\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "def do_perplexity_eval(args, model, data_file_path):\n",
        "  \"\"\"Computes the perplexity of the text in data_file_path according to the provided model.\"\"\"\n",
        "  set_seed(args.seed)\n",
        "\n",
        "  args.eval_data_file=data_file_path\n",
        "\n",
        "  _, _, tokenizer_class = run_language_modeling.MODEL_CLASSES[args.model_type]\n",
        "  tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  args.block_size = min(args.block_size, tokenizer.max_len)\n",
        "\n",
        "  result = run_language_modeling.evaluate(args, model, tokenizer, prefix=\"\")\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kClE2Px-j9bb",
        "colab_type": "text"
      },
      "source": [
        "#### Compute it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERCKSncEBYgJ",
        "colab_type": "code",
        "outputId": "4e9a910a-86e8-44df-ab3d-8a752285e07c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545,
          "referenced_widgets": [
            "0afe103e74bb405fa3d214be675fe6d2",
            "df95b38e933349bf91a36a28138ec041",
            "272bd060068046abb2b45b265980672a",
            "0c571c27ad254f29ba2d372be8af914a",
            "83318fe69b5142acb09c3b0be7ab4d98",
            "9ecef91390fc4be99706633e882e01cd",
            "3fe9f2a9b34d413281ffc51dd259cfdc",
            "c3d0f0c48c6a49a58beec7b4bb747c46",
            "e9d7aba0c8fa4d3ab48c2f07bb6eab83",
            "6d10126ecd8e43c4be054d0977f9d78d",
            "0ecf68fa3311440797cdc3543d5c3cbe",
            "323389e65a52469ab4217929cac4f7ad",
            "553acc9be7d84a73944a1f29fd69da4b",
            "8d924d58c1d34a6bb445b67e070df042",
            "7b9d36ee0764443f9e1a9b6da6ab42f1",
            "489abbf700f942929e21dbf97eff0c4e",
            "1f9df3aab26d40ac9cff9359546fa30a",
            "17f6224bafcb4d789b831ece60188bc8",
            "0831f963251f4fef922ce0eff014babc",
            "9d56503eaace42428dab014570fb46d2",
            "2e59b9c4599341298b95f9dc457394fe",
            "78dc39a0a3d446e085d6505e6f306001",
            "2be2a1b8c9e841b991cb824d19bbc0f7",
            "b9d29b44c7e44d079900fe5f05f1e553",
            "d8589186a82b4a75ae37217ca804c39c",
            "66e68150ae60499b82344151f6462c63",
            "9c28b17626f24817b89350fb7595d9fa",
            "bef601f75c8b473eb6aee7943d0497d1",
            "fe2c8c937bd5426c98b5109923975736",
            "b2f14080a31b46cf96bc2d5b5a2df66e",
            "ac1ecb4df5f1424db8dfafe0f8eead66",
            "6eca84f7e0864a95aa75c83830adddcd"
          ]
        }
      },
      "source": [
        "# Set this to the checkpoint you want to evalute, or to \"gpt2-medium\" to\n",
        "# evaluate the pre-trained model without finetuning.\n",
        "# CHECKPOINT_PATH = '/content/drive/My Drive/finetuned_model/checkpoint-1500'\n",
        "CHECKPOINT_PATH = \"gpt2-medium\"\n",
        "\n",
        "# Set this to the list of text files you want to evaluate the perplexity of.\n",
        "DATA_PATHS = [\"/content/presidential_speeches_valid.txt\",\n",
        "              \"/content/presidential_speeches_test.txt\"]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"Running on device: \", device)\n",
        "\n",
        "args = collections.defaultdict(\n",
        "  model_name_or_path=CHECKPOINT_PATH,\n",
        "  output_dir=CHECKPOINT_PATH,\n",
        "  block_size = 128,\n",
        "  local_rank=-1,\n",
        "  eval_batch_size=2,\n",
        "  per_gpu_eval_batch_size=2,\n",
        "  n_gpu=n_gpu,\n",
        "  mlm=False,\n",
        "  device=device,\n",
        "  line_by_line=False,\n",
        "  overwrite_cache=None,\n",
        "  model_type='gpt2',\n",
        "  seed=42,\n",
        ")\n",
        "args = DictToObj(args)\n",
        "\n",
        "model = load_model(args)\n",
        "\n",
        "for data_path in DATA_PATHS:\n",
        "  eval_results = do_perplexity_eval(args, model, data_path)\n",
        "  perplexity = eval_results['perplexity']\n",
        "  print('{} is the perplexity of {} according to {}'.format(\n",
        "      perplexity, data_path, CHECKPOINT_PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on device:  cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0afe103e74bb405fa3d214be675fe6d2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=341, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method tqdm.__del__ of Evaluating:  26%|â–ˆâ–ˆâ–‹       | 4176/15836 [07:40<14:20, 13.56it/s]>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\", line 931, in __del__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\", line 1133, in close\n",
            "    self._decr_instances(self)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\", line 496, in _decr_instances\n",
            "    cls.monitor.exit()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_monitor.py\", line 52, in exit\n",
            "    self.join()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1053, in join\n",
            "    raise RuntimeError(\"cannot join current thread\")\n",
            "RuntimeError: cannot join current thread\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9d7aba0c8fa4d3ab48c2f07bb6eab83",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=1520013706, style=ProgressStyle(descriptionâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f9df3aab26d40ac9cff9359546fa30a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=1042301, style=ProgressStyle(description_wiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8589186a82b4a75ae37217ca804c39c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=456318, style=ProgressStyle(description_widâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 496/496 [00:38<00:00, 14.47it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "23.510801315307617 is the perplexity of /content/presidential_speeches_valid.txt according to gpt2-medium\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 496/496 [00:35<00:00, 14.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "23.510801315307617 is the perplexity of /content/presidential_speeches_test.txt according to gpt2-medium\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5o7v2hmhMTO",
        "colab_type": "text"
      },
      "source": [
        "### Generate samples\n",
        "The following code generates text samples that are are continuations of a provided prompt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcvySe_wrCWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_samples(args, model, prompt_text):\n",
        "  \"\"\"Generating sampling for the provided prompt using the provided model.\"\"\"\n",
        "  set_seed(args.seed)\n",
        "\n",
        "  _, _, tokenizer_class = run_language_modeling.MODEL_CLASSES[args.model_type]\n",
        "  tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  requires_preprocessing = args.model_type in run_generation.PREPROCESSING_FUNCTIONS.keys()\n",
        "  encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  encoded_prompt = encoded_prompt.to(args.device)\n",
        "\n",
        "  output_sequences = model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=args.length + len(encoded_prompt[0]),\n",
        "      temperature=args.temperature,\n",
        "      top_k=args.k,\n",
        "      top_p=args.p,\n",
        "      repetition_penalty=args.repetition_penalty,\n",
        "      do_sample=True,\n",
        "      num_return_sequences=args.num_return_sequences,\n",
        "  )\n",
        "\n",
        "  # Remove the batch dimension when returning multiple sequences\n",
        "  if len(output_sequences.shape) > 2:\n",
        "    output_sequences.squeeze_()\n",
        "\n",
        "  generated_sequences = []\n",
        "\n",
        "  for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "    # Decode text\n",
        "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # Remove all text after the stop token\n",
        "    text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
        "\n",
        "    # Remove the excess text that was used for pre-processing\n",
        "    text = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "\n",
        "    # Add the prompt at the beginning of the sequence.\n",
        "    total_sequence = prompt_text + text\n",
        "\n",
        "    generated_sequences.append(total_sequence)\n",
        "\n",
        "  return generated_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3LKo9VVjHw0",
        "colab_type": "code",
        "outputId": "07c47ced-dccb-4b23-8bf8-c92911f2bfbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set this to the checkpoint you want to use for generation, or to \"gpt2-medium\"\n",
        "# to generate with the pre-trained model without finetuning.\n",
        "CHECKPOINT_PATH = '/content/drive/My Drive/finetuned_model/checkpoint-1500'\n",
        "\n",
        "# You should try out other prompts as well as no prompt at all.\n",
        "PROMPT = '<title=\\\"Remarks on Mission to Mars\\\">'\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"Running on device: \", device)\n",
        "\n",
        "args = collections.defaultdict(\n",
        "  model_name_or_path=CHECKPOINT_PATH,\n",
        "  output_dir=CHECKPOINT_PATH,\n",
        "  n_gpu=n_gpu,\n",
        "  mlm=False,\n",
        "  device=device,\n",
        "  model_type='gpt2',\n",
        "  seed=42,\n",
        "  stop_token=None, # Set this if your dataset has a special word that indicates the end of a text.\n",
        "  temperature=1.0,  # temperature sampling. Set this to temperature=1.0 to not use temperature.\n",
        "  k=50,  # k for top-k sampling. Set this to k=0 to not use top-k.\n",
        "  p=1.0,  # p for nucleus sampling. Set this to p=1.0 to not use nucleus sampling.\n",
        "  repetition_penalty=None,\n",
        "  length=100,  # Number of tokens to generate.\n",
        "  num_return_sequences=3,  # Number of independently computed samples to generate.\n",
        ")\n",
        "args = DictToObj(args)\n",
        "\n",
        "model = load_model(args)\n",
        "sequences = generate_samples(args, model, PROMPT)\n",
        "for sequence in sequences:\n",
        "  print(sequence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/22/2020 06:47:31 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_model/checkpoint-1500/config.json\n",
            "02/22/2020 06:47:31 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/22/2020 06:47:31 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/finetuned_model/checkpoint-1500/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running on device:  cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/22/2020 06:47:44 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/finetuned_model/checkpoint-1500' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/My Drive/finetuned_model/checkpoint-1500' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/22/2020 06:47:44 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/finetuned_model/checkpoint-1500/added_tokens.json. We won't load it.\n",
            "02/22/2020 06:47:44 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_model/checkpoint-1500/vocab.json\n",
            "02/22/2020 06:47:44 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_model/checkpoint-1500/merges.txt\n",
            "02/22/2020 06:47:44 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/22/2020 06:47:44 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_model/checkpoint-1500/special_tokens_map.json\n",
            "02/22/2020 06:47:44 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_model/checkpoint-1500/tokenizer_config.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "=== GENERATED SEQUENCE 1 ===\n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "<title=\"Remarks on Mission to Marks\">\n",
            "<president=\"polk\">\n",
            "<date=\"March 10, 1967\">\n",
            "The Senate and the House and the people of the United States:\n",
            "It is in their interest and in our interest to be here today to hear that the Cuban people know what has been required of them in return for their peaceful conversion to democratic communism. For over five years over 200,000 Cubans have lived and worked under the leadership a government which respects their autonomy and who has taken every possible step to protect them\n",
            "<title=\"Remarks on Mission to Marks\">\n",
            "<president=\"bush\">\n",
            "<date=\"April 14, 2003\">\n",
            "I'm pleased to be here today to say I'm one of 100th President of the United States. The United States of America is truly a great country, and the American people are even more extraordinary. In more than 60 years, more than 90 percent of your lives have been spent in this great house of the Presidency, in this place which was once called the White House. The vast majority of us have served in\n",
            "<title=\"Remarks on Mission to Marks\">\n",
            "<president=\"dawson\">\n",
            "<date=\"November 25, 1802\">\n",
            "My Fellow Americans, There is not, and never has been any nation in the history of the world that has pursued an industrial policy while the national resources were in great abundance, and the demand for labor was so greatly high. Every year there has been increased demand by the Government for labor, by legislation and otherwise, which made it imperative to do everything possible to get the surplus into the hands of the employers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfyWYdovtnBw",
        "colab_type": "text"
      },
      "source": [
        "# TODO: Instructions to give students\n",
        "* Play around with the presidential speech language model a bit. Compare its outputs to the statistical language model and the LSTM.\n",
        "* Create a dataset of your own. Find some text online and extract it into a .txt file. Ideally your dataset should contain between 10 and 50 MB of text.\n",
        "* Break your text file into 2 text files, one for training and one for testing. (We'll need to provide instructions on how to do this)\n",
        "* Report perplexity of your train set and your test set (we'll need to explain what perplexity is) on GPT-2 without finetuning and GPT-2 with finetuning. Perplexity should be lower after finetuning or something has gone wrong.\n",
        "\n",
        "# TODO: Analysis questions we could ask students\n",
        "* Generate 100 unconditioned samples from your finetuned model and 100 unconditioned examples from GPT-without finetuning. Do the samples from your finetuned model have higher word overlap with your test set.\n",
        "* What happens when you set top-k to 1 vs setting it to some large number?"
      ]
    }
  ]
}